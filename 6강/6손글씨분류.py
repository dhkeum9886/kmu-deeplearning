# -*- coding: utf-8 -*-
"""6손글씨분류.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QRu5Xt8lv3eyvlbOk4A5EHMSLazjJBJq
"""

# 손글씨분류
import matplotlib.pyplot as plt
from torchvision.datasets.mnist import MNIST
from torchvision.transforms import ToTensor

# 데이터셋 불러오기
training_data = MNIST(root='./', train=True, transform=ToTensor(), download=True)
test_data = MNIST(root='./', train=False, transform=ToTensor(), download=True) # train=False

print(len(training_data))
print(len(test_data))

for i in range(9):
  plt.subplot(3,3, i+1)
  plt.imshow(training_data.data[i])
plt.show()

# 데이터 로더
from torch.utils.data.dataloader import DataLoader

training_loader = DataLoader(training_data, batch_size=32, shuffle=True) # shuffle=True : 데이터를 섞어줌.
test_loader = DataLoader(test_data, batch_size=32, shuffle=False)
print(training_loader)
print(test_loader)

import torch
import torch.nn as nn
from torch.optim import Adam # Adam 옵티마이저

device = 'cuda' if torch.cuda.is_available() else 'cpu' # cuda 상태에 따라 device 선택

# nn.Sequential : 여러개의 layer 를 차례로 쌓아서 신경망 모델을 만들기 위한 함수, Sequential 안에 들어가는 layer는 순서대로 연결됨.
model=nn.Sequential(
    nn.Linear(28*28*1, 64), # Linear : MNIST 숫자 분류 문제,
                            # 28*28*1 -> 데이터셋의 이미지 사이즈, 흑백모드는 *1
                            # 64 -> 출력 벡터의 차원, 선형 계층을 통과한 후의 결과가 64개의 값(출력 뉴런, 또는 출력 feature)을 가지게 된다
    nn.ReLU(),              # ReLU : 0보다 작은 값은 0 으로, 0보다 크거나 같은 값은 입력값을 그대로 출력
    nn.Linear(64, 64),
    nn.ReLU(),
    nn.Linear(64, 10),
)
# 입력 뉴런 개수 28*28*1
# 출력 뉴런 개수 10
# 10개의 클래스 (0~9)로 분리하여 출력

model.to(device)

lr = 1e-3 #  학습률 : 10의 마이너스 3승, 0.001

optim=Adam(model.parameters(), lr=lr) # Adam 옵티마이저

for epoch in range(20):
  for data, label in training_loader: # 데이터, 레이블 => 정답을 의미, 지도 학습
    optim.zero_grad()                 # 기울기 0 값으로 초기화
    data=torch.reshape(data,(-1, 28*28*1)).to(device) # reshape(data,(-1, 28*28*1)) : 이미지 데이터를 1차원 벡터로 평탄화(flatten)하기 위함
    preds=model(data) # 실제로는 배치사이즈(32)*28*28*1 , (-1, 28*28*1) 하게 되면 (32, 28*28*1) 의미
    loss=nn.CrossEntropyLoss()(preds, label.to(device)) # CrossEntropyLoss : 분류(classification) 문제에서 자주 사용되는 손실 함수.
                                                        # 모델이 예측한 결과와 실제 레이블 간의 차이를 측정,
                                                        # 모델을 학습시킬 때 이 차이를 최소화하도록 가중치를 업데이트합
    loss.backward() # 역전파
    optim.step()
  print(f'epoch:{epoch+1}, loss:{loss.item()}')
torch.save(model.state_dict(), "MNIST.pth") # 모델 가중치를 토치딕셔너리 형태로 저장

model.load_state_dict(torch.load('MNIST.pth', map_location=device))

# 분류에 성공한 갯수, 0 초기화
num_corr = 0

# no_grad 기울기 계산하지 않음.
with torch.no_grad():
  for data, label in test_loader:
    data = torch.reshape(data, (-1, 28*28*1)).to(device)

    output=model(data.to(device))
    preds=output.data.max(1)[1] # 예측값 구하기,
                                # data 는 행렬, 행은 0번째 차원, 열은 1번째 차원,
                                # 열에서 데이터를 가지고 온다는 의미. max(1)은 1차원
                                # [1] -> [value, index] -> index
    corr=preds.eq(label.to(device).data).sum().item()
    num_corr+=corr
  print(f'Accuracy:{num_corr/len(test_data)}') # 정확도, 이상적인 범위는 95~98