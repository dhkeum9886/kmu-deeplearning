# 사인함수 예측하기 (딥러닝의 기초인 경사하강법을 사용)
# 이 코드는 임의로 생성한 3차 다항식을 이용하여 사인함수를 근사하는 과정을 보여줍니다.

import math
import torch
import matplotlib.pyplot as plt

# ---------------------------
# 1. 데이터 생성 및 준비
# ---------------------------

# -pi부터 +pi까지 1000개의 균일한 점을 생성합니다.
x = torch.linspace(-math.pi, math.pi, 1000)  # torch.linspace(시작값, 끝값, 개수)

# x에 해당하는 실제 사인값을 계산합니다.
y = torch.sin(x)  # 사인 함수의 결과를 y에 저장

# ---------------------------
# 2. 모델 파라미터 초기화
# ---------------------------

# 여기서는 3차 다항식을 사용하여 y값을 예측할 예정입니다.
# 모델은 y = a*x^3 + b*x^2 + c*x + d 형태를 가집니다.
# a, b, c, d는 모델이 학습을 통해 찾아낼 '가중치(계수)'입니다.
a = torch.randn(())  # 무작위 초기값을 생성 (정규분포를 따름)
b = torch.randn(())
c = torch.randn(())
d = torch.randn(())

# 초기 예측 결과 (학습 전)
# 현재 임의의 파라미터 값으로 계산된 다항식 결과를 y_random에 저장합니다.
y_random = a * x ** 3 + b * x ** 2 + c * x + d  # 3차 다항식 계산

# ---------------------------
# 3. 초기 결과 시각화
# ---------------------------
# 학습 전 실제 사인함수와 모델의 예측(임의의 다항식)을 비교합니다.

# 첫 번째 subplot: 실제 사인함수
plt.subplot(2, 1, 1)
plt.title('y true')
plt.plot(x, y)  # x에 대한 사인값 그래프

# 두 번째 subplot: 초기 모델 예측 (임의의 다항식)
plt.subplot(2, 1, 2)
plt.title('y random')
plt.plot(x, y_random)  # 초기 가중치로 계산된 다항식 그래프

# 그래프 창을 띄워서 결과를 확인합니다.
plt.show()

# ---------------------------
# 4. 학습 준비: 경사하강법 설정
# ---------------------------

# 학습률(learning rate)은 파라미터를 업데이트할 때 한 번에 얼마나 이동할지를 결정합니다.
learning_rate = 1e-6  # 매우 작은 값(0.000001): 너무 크게 업데이트하면 최적점을 지나칠 수 있습니다.

# ---------------------------
# 5. 모델 학습 (경사하강법을 사용하여 파라미터 최적화)
# ---------------------------

# 에포크(epoch): 전체 데이터셋을 한 번 사용하여 학습하는 과정을 의미하며, 여기서는 2000번 반복합니다.
for epoch in range(2000):

    # 모델 정의: 현재 파라미터 값으로 예측 y값을 계산합니다.
    # y_pred는 현재 모델이 예측한 값으로, 3차 다항식에 x를 대입한 결과입니다.
    y_pred = (a * x ** 3) + (b * x ** 2) + (c * x) + d

    # 손실 함수 계산:
    # 손실(loss)은 예측값(y_pred)과 실제값(y) 사이의 차이를 나타내며,
    # 여기서는 차이를 제곱한 값들의 합을 사용합니다.
    loss = (y_pred - y).pow(2).sum().item()
    # (y_pred - y): 예측과 실제값의 차이 (오차)
    # .pow(2): 오차를 제곱하여 음수가 나오지 않게 함
    # .sum(): 모든 오차 제곱의 총합
    # .item(): 텐서 값을 파이썬 숫자로 변환 (출력용)

    # 100 에포크마다 현재 에포크 번호와 손실 값을 출력하여 학습 과정을 모니터링합니다.
    if epoch % 100 == 0:
        print(f"epoch{epoch + 1} loss: {loss}")

    # ---------------------------
    # 6. 역전파(Backpropagation)를 통한 기울기(gradient) 계산
    # ---------------------------
    # 역전파는 손실 함수의 값을 줄이기 위해 각 파라미터가 어떻게 변경되어야 하는지 계산하는 과정입니다.

    # 손실 함수 L = (y_pred - y)^2의 미분은 2*(y_pred - y)입니다.
    grad_y_pred = 2.0 * (y_pred - y)

    # 각 파라미터(a, b, c, d)별로 체인 룰을 적용해 기울기를 계산합니다.
    # 예를 들어, y_pred가 a에 대해 미분되면 x^3가 되므로, 전체 기울기는 2*(y_pred - y)*x^3의 합입니다.
    grad_a = (grad_y_pred * x ** 3).sum()  # a의 기울기: 각 데이터에서 x^3에 해당하는 부분을 곱한 후 모두 더함
    grad_b = (grad_y_pred * x ** 2).sum()  # b의 기울기: x^2 항
    grad_c = (grad_y_pred * x).sum()  # c의 기울기: x 항
    grad_d = grad_y_pred.sum()  # d의 기울기: 상수항이므로 x에 상관없이 단순 합산

    # ---------------------------
    # 7. 파라미터 업데이트 (경사하강법)
    # ---------------------------
    # 계산된 기울기를 사용해 파라미터를 조정합니다.
    # 업데이트 규칙: 새로운 파라미터 = 기존 파라미터 - (학습률 * 기울기)
    a -= learning_rate * grad_a
    b -= learning_rate * grad_b
    c -= learning_rate * grad_c
    d -= learning_rate * grad_d

# ---------------------------
# 8. 학습 후 결과 시각화
# ---------------------------
# 학습이 끝난 후, 실제 사인함수, 학습된 모델의 예측 결과, 그리고 초기 예측 결과를 비교하여 모델의 성능 변화를 확인합니다.

# 첫 번째 subplot: 실제 사인 함수 그래프
plt.subplot(3, 1, 1)
plt.title('y true')
plt.plot(x, y)

# 두 번째 subplot: 학습 후 모델의 예측 그래프
plt.subplot(3, 1, 2)
plt.title('y pred')
plt.plot(x, y_pred)

# 세 번째 subplot: 초기 모델 예측 그래프 (학습 전 결과)
plt.subplot(3, 1, 3)
plt.title('y random')
plt.plot(y_random)

# 최종 결과를 확인합니다.
plt.show()
